# ğŸ–¼ï¸ Visual Question Answering (VQA) Model

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-orange.svg)](https://pytorch.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

A deep learning model for Visual Question Answering that combines visual and textual understanding to answer questions about images. Built with PyTorch, leveraging ResNet50 for image encoding and BERT for text encoding with a novel gated fusion mechanism.

---

## ğŸ“Š Model Performance

| Dataset | Hard Accuracy | Soft Accuracy (VQA Standard) |
|---------|--------------|------------------------------|
| **Validation** | **49.90%** | **58.56%** |

> **Note**: Soft Accuracy uses the official VQA evaluation metric: `min(#humans_who_gave_answer / 3, 1)`

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Input Image   â”‚     â”‚  Input Question â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ResNet50      â”‚     â”‚  BERT Encoder   â”‚
â”‚ (Image Encoder) â”‚     â”‚ (Text Encoder)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
              â”‚  Attention  â”‚
              â”‚   Module    â”‚
              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
              â”‚   Gated     â”‚
              â”‚   Fusion    â”‚
              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
              â”‚ Classifier  â”‚
              â”‚  (FC Layer) â”‚
              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Answer    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

|     Component     |                 Description                 |
|-------------------|---------------------------------------------|
| **Image Encoder** |      ResNet50 (pretrained on ImageNet)      |
|  **Text Encoder** |              BERT-base-uncased              |
|   **Attention**   |   Visual attention guided by text features  |
|   **Fusion**      |     Gated fusion mechanism with dropout     |
|   **Classifier**  | Fully connected layer (1000 answer classes) |

---

## ğŸ“ Project Structure

```
VQAModel/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default_config.yaml    # Hyperparameters & paths
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ vqa_dataset.py         # Dataset loader
â”‚   â””â”€â”€ answer_to_idx.json     # Answer vocabulary
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ vqa_model.py           # Main VQA model
â”‚   â”œâ”€â”€ encoders.py            # Image & Text encoders
â”‚   â”œâ”€â”€ fusion.py              # Gated fusion module
â”‚   â””â”€â”€ attention.py           # Attention mechanism
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py               # Training script
â”‚   â”œâ”€â”€ evaluate.py            # Evaluation script
â”‚   â”œâ”€â”€ generate_submission.py # EvalAI submission generator
â”‚   â””â”€â”€ build_vocab.py         # Answer vocabulary builder
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ metrics.py             # VQA soft accuracy metric
â”œâ”€â”€ checkpoints/               # Saved model weights
â”œâ”€â”€ notebooks/                 # Jupyter notebooks
â””â”€â”€ dataset/                   # VQA v2.0 dataset
```

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.10+
- CUDA-compatible GPU (recommended)
- 16GB+ RAM

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/princ3kr/VQAModel.git
   cd VQAModel
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Download VQA v2.0 Dataset**
   
   Download the following from [VQA v2.0 website](https://visualqa.org/download.html):
   - Training images (COCO 2014)
   - Validation images (COCO 2014)
   - Training questions
   - Validation questions
   - Training annotations
   - Validation annotations

   Place them in the `dataset/coco2014/` directory following this structure:
   ```
   dataset/coco2014/
   â”œâ”€â”€ images/
   â”‚   â”œâ”€â”€ train2014/
   â”‚   â”œâ”€â”€ val2014/
   â”‚   â””â”€â”€ test2014/
   â”œâ”€â”€ questions/
   â”‚   â”œâ”€â”€ OpenEnded_mscoco_train2014_questions.json
   â”‚   â”œâ”€â”€ OpenEnded_mscoco_val2014_questions.json
   â”‚   â””â”€â”€ OpenEnded_mscoco_test2015_questions.json
   â””â”€â”€ annotations/
       â”œâ”€â”€ mscoco_train2014_annotations.json
       â””â”€â”€ mscoco_val2014_annotations.json
   ```

---

## ğŸ‹ï¸ Training

### Build Answer Vocabulary (First Time Only)
```bash
python scripts/build_vocab.py
```

### Train the Model
```bash
python scripts/train.py
```

### Configuration

Edit `configs/default_config.yaml` to customize training:

```yaml
model:
  image_encoder:
    model_name: "resnet50"
    frozen: true
  text_encoder:
    model_name: "bert-base-uncased"
    frozen: false
  fusion:
    hidden_size: 1024
    dropout: 0.5
  output_size: 1000

training:
  batch_size: 16
  epochs: 10
  learning_rate: 0.0001
  save_dir: "checkpoints/"
```

---

## ğŸ“ˆ Evaluation

### Evaluate on Validation Set
```bash
python scripts/evaluate.py
```

### Generate EvalAI Submission (Test Set)
```bash
python scripts/generate_submission.py
```

The submission file will be saved to `checkpoints/vqa_submission.json`.

---

## ğŸ“‹ Results Interpretation

The model uses two accuracy metrics:

- **Hard Accuracy**: Exact match with the most common ground truth answer
- **Soft Accuracy (VQA Standard)**: `min(#annotators_who_gave_answer / 3, 1)`
  - If 0 annotators gave the predicted answer: 0%
  - If 1 annotator gave the predicted answer: 33.3%
  - If 2 annotators gave the predicted answer: 66.7%
  - If 3+ annotators gave the predicted answer: 100%

---

## ğŸ”§ Technical Details

### Hardware Requirements
- **GPU**: NVIDIA GPU with 8GB+ VRAM (training)
- **RAM**: 16GB+ recommended
- **Storage**: ~25GB for dataset

### Training Details
- **Optimizer**: Adam
- **Learning Rate**: 1e-4
- **Batch Size**: 16-32
- **Image Size**: 224Ã—224
- **Max Question Length**: 30 tokens

---

## ğŸ“š References

- [VQA: Visual Question Answering](https://arxiv.org/abs/1505.00468) - Agrawal et al.
- [Making the V in VQA Matter](https://arxiv.org/abs/1612.00837) - Goyal et al.
- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805) - Devlin et al.
- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) - He et al.

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“§ Contact

For questions or feedback, please open an issue on GitHub.

---

<p align="center">
  Made with â¤ï¸ using PyTorch
</p>
