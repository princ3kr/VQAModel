{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "acf97fde-36c0-4901-bd3d-540af9d678e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6eb2965b-93ee-4550-8e2e-e4ee64a2fa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "96985981-8415-4a91-b0e0-3206532d06b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQAmodel(nn.Module):\n",
    "    def __init__(self, vocab_size, num_ans):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.img_encoder = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        \n",
    "        for name, param in self.img_encoder.named_parameters():\n",
    "            if 'layer3' not in name and 'layer4' not in name:\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.img_fc = nn.Sequential(\n",
    "            nn.Linear(512, 768),\n",
    "            nn.BatchNorm1d(768),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.ques_encoder = nn.Embedding(vocab_size, 300)\n",
    "        self.lstm = nn.LSTM(300, 768, num_layers=2, batch_first=True, dropout=0.3)\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(768*2, 768),\n",
    "            nn.BatchNorm1d(768),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 384),\n",
    "            nn.BatchNorm1d(384),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(384, num_ans)\n",
    "        )\n",
    "    \n",
    "    def forward(self, image, question):\n",
    "        img_encod = self.img_encoder(image)\n",
    "        img_encod = img_encod.view(img_encod.size(0), -1)\n",
    "        img_encod = self.img_fc(img_encod)\n",
    "        \n",
    "        q_encod = self.ques_encoder(question)\n",
    "        _, (q_features, _) = self.lstm(q_encod)\n",
    "        q_features = q_features[-1]\n",
    "        \n",
    "        combined = torch.cat([img_encod, q_features], dim=1)\n",
    "        fused = self.fusion(combined)\n",
    "        \n",
    "        result = self.classifier(fused)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d94318cf-65ff-4991-b4e4-69a15653a5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, question, answer, image_path, transform):\n",
    "        super().__init__()\n",
    "        self.question = question\n",
    "        self.answer = answer\n",
    "        self.image_path = image_path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.question)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_path[idx]).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        question = torch.tensor(self.question[idx], dtype=torch.long)\n",
    "        answer = torch.tensor(self.answer[idx], dtype=torch.long)\n",
    "        \n",
    "        return image, question, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5c0f0ed2-3fb6-4306-8575-00dc47066342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original samples: 9974\n",
      "Filtered samples: 7525\n",
      "Answer classes reduced to 120\n",
      "Samples per class: 62.7\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('dataset//data_train.csv')\n",
    "ques = train_data.iloc[:, 0].tolist()\n",
    "imgs = train_data.iloc[:, 2].tolist()\n",
    "ans = train_data.iloc[:, 1].tolist()\n",
    "\n",
    "answer_counts = Counter(ans)\n",
    "top_answers = [a for a, _ in answer_counts.most_common(100)]\n",
    "\n",
    "filtered_data = [(q, a, img) for q, a, img in zip(ques, ans, imgs) if a in top_answers]\n",
    "ques = [x[0] for x in filtered_data]\n",
    "ans = [x[1] for x in filtered_data]\n",
    "imgs = [x[2] for x in filtered_data]\n",
    "\n",
    "print(f\"Original samples: {len(train_data)}\")\n",
    "print(f\"Filtered samples: {len(ques)}\")\n",
    "print(f\"Answer classes reduced to 120\")\n",
    "print(f\"Samples per class: {len(ques)/120:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6ba5df5a-e309-4f80-b6c5-a86b0333caad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(sentences):\n",
    "    words = []\n",
    "    for line in sentences:\n",
    "        word = line.lower().strip().split()\n",
    "        words.extend(word)\n",
    "    words = ['<PAD>', '<UNK>'] + sorted(set(words))\n",
    "    word_to_idx = {word: i for i, word in enumerate(words)}\n",
    "    return word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b8eeb48f-288c-4f7d-b0d8-3dcf0f10a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(vocab, sentences, max_len=25):\n",
    "    tokenized = []\n",
    "    for line in sentences:\n",
    "        ques = []\n",
    "        words = line.lower().strip().split()\n",
    "        for word in words:\n",
    "            if word in vocab:\n",
    "                ques.append(vocab[word])\n",
    "            else:\n",
    "                ques.append(vocab['<UNK>'])\n",
    "        \n",
    "        if len(ques) < max_len:\n",
    "            ques += [vocab['<PAD>']] * (max_len - len(ques))\n",
    "        else:\n",
    "            ques = ques[:max_len]\n",
    "        \n",
    "        tokenized.append(ques)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a3b92e07-2779-4b77-b05e-37c76054d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = ['dataset//images//' + img + '.png' for img in imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3732af84-495c-46a4-b302-545f40816ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 888\n",
      "Answer classes: 100\n"
     ]
    }
   ],
   "source": [
    "vocab = make_vocab(ques)\n",
    "vocab_size = len(vocab)\n",
    "tokenized_ques = tokenize(vocab, ques)\n",
    "\n",
    "unique_answers = sorted(set(ans))\n",
    "ans_to_idx = {answer: idx for idx, answer in enumerate(unique_answers)}\n",
    "num_ans = len(ans_to_idx)\n",
    "tokenized_ans = [ans_to_idx[a] for a in ans]\n",
    "\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(f\"Answer classes: {num_ans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e00cf8f3-158a-4258-b2ac-a613b34c2343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 6396\n",
      "Val samples: 1129\n"
     ]
    }
   ],
   "source": [
    "train_idx, val_idx = train_test_split(range(len(tokenized_ques)), test_size=0.15, random_state=42)\n",
    "\n",
    "train_ques = [tokenized_ques[i] for i in train_idx]\n",
    "train_ans = [tokenized_ans[i] for i in train_idx]\n",
    "train_imgs = [image_path[i] for i in train_idx]\n",
    "\n",
    "val_ques = [tokenized_ques[i] for i in val_idx]\n",
    "val_ans = [tokenized_ans[i] for i in val_idx]\n",
    "val_imgs = [image_path[i] for i in val_idx]\n",
    "\n",
    "train_dataset = CustomDataset(train_ques, train_ans, train_imgs, train_transforms)\n",
    "val_dataset = CustomDataset(val_ques, val_ans, val_imgs, test_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e35d8095-fd14-4647-8d30-a41e0c61f3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "20e75dc3-8a01-48fb-86d7-962b6e63f72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what is the object on the shelves\n",
      "Answer: cup\n",
      "Image: dataset//images//image100.png\n",
      "\n",
      "Tokenized question: [866, 422, 785, 519, 524, 785, 681, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Tokenized answer: 34\n",
      "Answer should be in range [0, 99]\n"
     ]
    }
   ],
   "source": [
    "# Verify one sample\n",
    "idx = 0\n",
    "print(f\"Question: {ques[idx]}\")\n",
    "print(f\"Answer: {ans[idx]}\")\n",
    "print(f\"Image: {image_path[idx]}\")\n",
    "print(f\"\\nTokenized question: {tokenized_ques[idx]}\")\n",
    "print(f\"Tokenized answer: {tokenized_ans[idx]}\")\n",
    "print(f\"Answer should be in range [0, {num_ans-1}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "51fb1030-aef1-4963-bc18-849993877759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRINCE KUMAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\PRINCE KUMAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\PRINCE KUMAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using: {device}\")\n",
    "\n",
    "model = VQAmodel(vocab_size, num_ans).to(device)\n",
    "model.train()\n",
    "\n",
    "criterion_loss = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b6f81f8c-0669-4b8e-bbc9-dce56d8f1be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRINCE KUMAR\\AppData\\Local\\Temp\\ipykernel_31892\\1898523544.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "C:\\Users\\PRINCE KUMAR\\AppData\\Local\\Temp\\ipykernel_31892\\1898523544.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "C:\\Users\\PRINCE KUMAR\\AppData\\Local\\Temp\\ipykernel_31892\\1898523544.py:46: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: Train Loss=4.4186 Acc=4.71% | Val Loss=4.1790 Acc=8.06%\n",
      "Best model saved\n",
      "Epoch  2: Train Loss=3.9896 Acc=10.58% | Val Loss=3.8161 Acc=11.51%\n",
      "Best model saved\n",
      "Epoch  3: Train Loss=3.7872 Acc=12.54% | Val Loss=3.7751 Acc=13.11%\n",
      "Best model saved\n",
      "Epoch  4: Train Loss=3.5806 Acc=16.39% | Val Loss=3.4512 Acc=17.36%\n",
      "Best model saved\n",
      "Epoch  5: Train Loss=3.3937 Acc=19.82% | Val Loss=3.4094 Acc=17.89%\n",
      "Best model saved\n",
      "Epoch  6: Train Loss=3.2780 Acc=21.62% | Val Loss=3.3376 Acc=21.26%\n",
      "Best model saved\n",
      "Epoch  7: Train Loss=3.2135 Acc=22.53% | Val Loss=3.2749 Acc=20.55%\n",
      "Best model saved\n",
      "Epoch  8: Train Loss=3.1346 Acc=25.89% | Val Loss=3.1869 Acc=23.56%\n",
      "Best model saved\n",
      "Epoch  9: Train Loss=3.0471 Acc=27.99% | Val Loss=3.1217 Acc=25.42%\n",
      "Best model saved\n",
      "Epoch 10: Train Loss=2.9724 Acc=29.24% | Val Loss=3.1329 Acc=26.22%\n",
      "  Patience: 1/7\n",
      "Epoch 11: Train Loss=2.8953 Acc=31.94% | Val Loss=3.0975 Acc=27.28%\n",
      "Best model saved\n",
      "Epoch 12: Train Loss=2.8451 Acc=33.61% | Val Loss=3.0402 Acc=28.88%\n",
      "Best model saved\n",
      "Epoch 13: Train Loss=2.7882 Acc=35.18% | Val Loss=3.0315 Acc=28.96%\n",
      "Best model saved\n",
      "Epoch 14: Train Loss=2.7067 Acc=37.18% | Val Loss=3.0318 Acc=29.85%\n",
      "  Patience: 1/7\n",
      "Epoch 15: Train Loss=2.6587 Acc=38.98% | Val Loss=3.0267 Acc=29.41%\n",
      "Best model saved\n",
      "Epoch 16: Train Loss=2.5990 Acc=40.40% | Val Loss=3.0366 Acc=31.36%\n",
      "  Patience: 1/7\n",
      "Epoch 17: Train Loss=2.5380 Acc=42.37% | Val Loss=3.0480 Acc=29.85%\n",
      "  Patience: 2/7\n",
      "Epoch 18: Train Loss=2.4926 Acc=44.14% | Val Loss=3.0400 Acc=30.91%\n",
      "  Patience: 3/7\n",
      "Epoch 19: Train Loss=2.4516 Acc=45.36% | Val Loss=3.0513 Acc=31.00%\n",
      "  Patience: 4/7\n",
      "Epoch 20: Train Loss=2.3464 Acc=48.75% | Val Loss=3.0469 Acc=32.33%\n",
      "  Patience: 5/7\n",
      "Epoch 21: Train Loss=2.2735 Acc=51.45% | Val Loss=3.0397 Acc=31.80%\n",
      "  Patience: 6/7\n",
      "Epoch 22: Train Loss=2.2320 Acc=52.88% | Val Loss=3.0694 Acc=31.53%\n",
      "  Patience: 7/7\n",
      "Early stopping!\n",
      "\n",
      " Training Complete!\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler()\n",
    "best_val_loss = float('inf')\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (images, questions, answers) in enumerate(train_loader):\n",
    "        images, questions, answers = images.to(device), questions.to(device), answers.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model(images, questions)\n",
    "            loss = criterion_loss(outputs, answers)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += answers.size(0)\n",
    "        correct += (predicted == answers).sum().item()\n",
    "    \n",
    "    train_loss = epoch_loss / len(train_loader)\n",
    "    train_acc = 100 * correct / total\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, questions, answers in val_loader:\n",
    "            images, questions, answers = images.to(device), questions.to(device), answers.to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(images, questions)\n",
    "                loss = criterion_loss(outputs, answers)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += answers.size(0)\n",
    "            val_correct += (predicted == answers).sum().item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    \n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch+1:2d}: Train Loss={train_loss:.4f} Acc={train_acc:.2f}% | Val Loss={avg_val_loss:.4f} Acc={val_acc:.2f}%')\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'vqa_best_Bigger_Model.pth')\n",
    "        print('Best model saved')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f'  Patience: {patience_counter}/{patience}')\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "\n",
    "print('\\n Training Complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3aed0b9a-33b7-4e0d-88ef-a49b48098a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test samples: 2494\n",
      "Valid samples: 1864\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('dataset//data_eval.csv')\n",
    "test_ques = test_df.iloc[:, 0].tolist()\n",
    "test_ans = test_df.iloc[:, 1].tolist()\n",
    "test_imgs = test_df.iloc[:, 2].tolist()\n",
    "\n",
    "test_tokenized_ques = tokenize(vocab, test_ques, max_len=25)\n",
    "test_tokenized_ans = []\n",
    "valid_indices = []\n",
    "\n",
    "for idx, a in enumerate(test_ans):\n",
    "    if a in ans_to_idx:\n",
    "        test_tokenized_ans.append(ans_to_idx[a])\n",
    "        valid_indices.append(idx)\n",
    "\n",
    "test_tokenized_ques_filtered = [test_tokenized_ques[i] for i in valid_indices]\n",
    "test_imgs_filtered = [test_imgs[i] for i in valid_indices]\n",
    "test_image_path = ['dataset//images//' + img + '.png' for img in test_imgs_filtered]\n",
    "\n",
    "test_dataset = CustomDataset(test_tokenized_ques_filtered, test_tokenized_ans, test_image_path, test_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, pin_memory=True)\n",
    "\n",
    "print(f\"Total test samples: {len(test_ans)}\")\n",
    "print(f\"Valid samples: {len(valid_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181c78d4-de0c-47b6-a58c-48d248ed3910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRINCE KUMAR\\AppData\\Local\\Temp\\ipykernel_31892\\1641909204.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL TEST RESULTS:\n",
      "  Loss: 3.0232\n",
      "  Accuracy: 28.97%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('vqa_best_Bigger_Model.pth', map_location=device, weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "test_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, questions, answers in test_loader:\n",
    "        images, questions, answers = images.to(device), questions.to(device), answers.to(device)\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model(images, questions)\n",
    "            loss = criterion_loss(outputs, answers)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_total += answers.size(0)\n",
    "        test_correct += (predicted == answers).sum().item()\n",
    "\n",
    "test_acc = 100 * test_correct / test_total\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'FINAL TEST RESULTS:')\n",
    "print(f'  Loss: {avg_test_loss:.4f}')\n",
    "print(f'  Accuracy: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8493f036-36f1-4c7b-a121-9cd0bd8508e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
